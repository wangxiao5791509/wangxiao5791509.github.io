<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiao Wang</title>
<style>
    .container {
        display: flex;
        justify-content: space-between;
    }

    .imgtable img {
        align-self: center;
    }
</style>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Xiao Wang (安徽大学 王逍)</h1>
</div>
<table class="imgtable">
  <tr><td><a href="https://wangxiao5791509.github.io/"><img src="picture/xiaowang.JPG" alt="alt text" width="140px" /></a>&nbsp;	</td>
  <td align="left"><p>Associate Professor (副教授)<br /> 
  	School of Computer Science and Technology, Anhui University, Hefei 230601, China <br /> 
  	Email: wangxiaocvpr@foxmail.com; xiaowang@ahu.edu.cn <br /> 
<br /> 
<a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F5y9bxxP-FRWEf7aiRuT51F8TV9OiT1uk7E-Ak0IMOTvmHoW6B5gJwLsBJ6tkj1XunyQWh2rhbfQLf-cUo6gZb6mgFO33k9D_qfPqCTyivEjOmXZuM&user=oq9awGMAAAAJ">[Google Scholar]</a> 
<a href="https://github.com/wangxiao5791509">[GitHub]</a> 
<a href="https://dblp.uni-trier.de/pid/49/67-14.html">[DBLP]</a>
<a href="https://orcid.org/0000-0001-6117-6745">[ORCID]</a>
<a href="https://www.cnblogs.com/wangxiaocvpr/">[博客园]</a>
<a href="https://www.zhihu.com/people/wangxiaocvpr">[知乎]</a>
<a href="https://weibo.com/5070353058">[新浪微博]</a>
<a href="https://www.youtube.com/channel/UCSmlgXQRqFsUI--mQm90JwQ">[YouTube]</a>
<a href="https://github.com/Event-AHU">[Event-AHU]</a> 
<a href="https://cs.ahu.edu.cn/2023/0222/c20807a301355/page.htm">[安徽大学-计算机学院官网-个人简介]</a> 
<a href="https://ahu-spvl.github.io/">[安徽大学—结构模式与视觉学习研究组]</a>
<a href="https://cs.ahu.edu.cn/main.htm">[安徽大学-计算机学院官网]</a> 
</td>









</tr></table>

<h2>Biography</h2>
<p>Xiao Wang (Member, IEEE) received the B.S. degree from West Anhui University, Luan, China, in 2013, 
  and the Ph.D. degree in computer science from <a href="http://cs.ahu.edu.cn/">[<b>Anhui University</b>]</a>, Hefei, China, in 2019. 
  In his PhD career, he was supervised by Professor 
	<a href="https://cs.ahu.edu.cn/2021/1214/c20806a276961/page.htm">[<b>Jin Tang (汤进教授)</b>]</a> and 
	<a href="https://cs.ahu.edu.cn/2022/0307/c20806a280841/page.htm">[<b>Bin Luo (罗斌教授)</b>]</a> 
  (Lab: <a href="https://mcc.ahu.edu.cn/">[<b>Multi-Modal Intelligent Computing Group</b>]</a>). 
  From 2015 and 2016, he was a visiting student with the School of Data and Computer Science, <b>Sun Yat-sen University</b>, Guangzhou, China, 
  and supervised by Professor <a href="http://www.linliang.net/">[<b>Liang Lin (林倞教授)</b>]</a>. 
  He visited UBTECH Sydney Artificial Intelligence Centre, the Faculty of Engineering, <b>The University of Sydney</b>, Sydney, NSW, Australia, 
  in 2019, and supervised by Professor <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">[<b>Dacheng Tao (陶大程教授)</b>]</a>. 
  He finished the Postdoctoral Research (from April 2020 to April 2022) with <a href="https://www.pcl.ac.cn/">[<b>Pengcheng Laboratory</b>]</a> (Shenzhen, China) 
  and cooperate with Professor 
  <a href="https://eeis.ustc.edu.cn/2014/0423/c2648a20109/page.htm">[<b>Feng Wu (吴枫教授)</b>]</a>, 
  <a href="https://www.pkuml.org/staff/yhtian.html">[<b>Yonghong Tian (田永鸿教授)</b>]</a>, and 
  <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ&hl=zh-CN&oi=ao">[<b>Yaowei Wang (王耀威教授)</b>]</a>. 
  Currently, he is working at the <a href="http://cs.ahu.edu.cn/">[School of Computer Science and Technology]</a> of <b>Anhui University</b>, 
  Hefei 230601, China, as an Associate Professor. 

<p>His current research interests are mainly computer vision, event-based vision, machine learning, and pattern recognition. 
  Dr. Wang serves as an Associate Editor for the journal "IEEE Sensors Journal" (Impact Factor 4.3, JCR Q1, SCI Q2), and also 
  a reviewer for a number of journals and conferences, such as the International Journal of Computer Vision, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, 
  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, IEEE TRANSACTIONS ON IMAGE PROCESSING, 
  Computer Vision and Image Understanding, CVPR, ICCV, ECCV, AAAI, ACCV, ACM-MM, and WACV. 
  For more information, see: https://wangxiao5791509.github.io/.</p>

<p>王逍，博士，安徽大学计算机科学与技术学院副教授。于2019年获得安徽大学计算机科学与技术学院博士学位。在硕博连读期间，由汤进教授、罗斌教授指导。
  2015年至2016年，在广州中山大学数据与计算机科学学院作为访问学生，受到林倞教授的指导。
  2019年，访问了悉尼大学工程学院UBTECH悉尼人工智能中心，受到陶大程教授的指导。
  荣获2019年度安徽省省级优秀毕业生。
  2020年至2022年在鹏城实验室(深圳)从事博士后研究，与中国科学技术大学吴枫教授、北京大学田永鸿教授和王耀威教授合作。
  目前，在安徽大学计算机科学与技术学院担任副教授，从事教学和科研工作，主要研究兴趣包括计算机视觉、事件驱动的视觉、机器学习和模式识别。
  在国内外高水平会议期刊发表论文 40 余篇，包括 CCF-A 类会议（CVPR、AAAI），国际顶级期刊（TIP、TNNLS、TMM、TCSVT、TCYB、TGRS、PR等），
  谷歌学术引用达 2000+ 次。
  担任《IEEE Sensors Journal》(IF 4.3, JCR Q1, SCI 二区)期刊编委（Associate Editor）;
  担任主流期刊和会议的审稿人，包括IJCV, IEEE TIP, TNNLS, TMM, TCSVT, CVPR, ICCV, ECCV, AAAI, ACM-MM等。
  主持国家自然科学基金（青年项目）、博士后博新计划、博士后面上项目、安徽省自然科学基金（优青项目）等。
  2022年入选安徽省合肥市D类高层次人才(市级领军人才)，
  2023年荣获 ACM-合肥分会-“新星奖”(ACM CHINA COUNCIL HEFEI CHAPTER RISING STAR, No.2023ACMCHINA-XX-C1002) 。
  

<p><font color="#FF0000"><b>欢迎对科研感兴趣，编程能力强，基础扎实，善于思考和钻研，认真负责的学生报考本人研究生；欢迎学有余力的本科生参与科研项目实践；有意者请随时与我联系! (划水、混学位者勿扰!) </b></font> <br /> 
  
  

   
  
  
<h2>Research Topic</h2>
<ul>
<li><p><b>Computer Vision and Pattern Recognition</b>: Object Detection and Tracking, Segmentation, Attribute Recognition, etc. 
<li><p><b>Multi-modal and Multi-media</b>: Vision-Language, Vision-Thermal (Infrared), Vision-Depth, Vision-Event, etc.
<li><p><b>Event-based Vision</b>: Detection and Tracking, Action Recognition, etc.  
<li><p><b>AI4Science</b>: AI+Medical Image Analysis, AI+Controlled Nuclear Fusion, etc. 
</ul>



<h2>Research Team Members</h2> 
<style>
  .student-list ul {
    list-style: none; 
    width: 700px; 
	 padding-left: 40px;
  }
  .student-list li {
    display: flex; 
    justify-content: space-between;
    margin: 4px 60px; 
	
  }
  .student-list span.name {
    flex: 1; 
    text-align: justify; 
    text-align-last: justify; 
    margin: 0 15px;
  }
</style>
	
<ul>
<span><p><li> PhD Students：
 	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &loz; Admission Year 2024: 王世傲&nbsp;&nbsp;&nbsp;金建东&nbsp;&nbsp;&nbsp;吴文滔 </p></span> 
	
<span><p><li> Master Students：
	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &loz; Admission Year 2022: 戎耀&nbsp;&nbsp;&nbsp;黄菊&nbsp;&nbsp;&nbsp;李冬&nbsp;&nbsp;&nbsp;杨浩翔  
 	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &loz; Admission Year 2023: 朱倩&nbsp;&nbsp;&nbsp;李越航&nbsp;&nbsp;&nbsp;王福灵&nbsp;&nbsp;&nbsp;金宇&nbsp;&nbsp;&nbsp;王超&nbsp;&nbsp;&nbsp;王海洋&nbsp;&nbsp;&nbsp;姜精涛&nbsp;&nbsp;&nbsp;孔维哲&nbsp;&nbsp;&nbsp;陈强  
 	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &loz; Admission Year 2024: 楼旭锋&nbsp;&nbsp;&nbsp;胡塘&nbsp;&nbsp;&nbsp;金李烨&nbsp;&nbsp;&nbsp;吴淑娟&nbsp;&nbsp;&nbsp;王子文&nbsp;&nbsp;&nbsp;司昊&nbsp;&nbsp;&nbsp;翁朝流&nbsp;&nbsp;&nbsp;王梦琪&nbsp;&nbsp;&nbsp;乔雨晗&nbsp;&nbsp;&nbsp;张帆 </p></span> 
     
<span><p><li> Graduated Students：
	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &loz; Academic Year 2021-2024: 吴宗振&nbsp;&nbsp;&nbsp;王子天&nbsp;&nbsp;&nbsp;袁程果 </p></span> 
</ul>












	
<h2>Selected Publications</h2> 


<ul> 
<li><p><a href="">HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with Heterogeneous Graph Adapter</a> <br />
Yumiao Zhao, Bo Jiang, <b>Xiao Wang</b>, Qin Xu, Jin Tang <br />
<i> arXiv Pre-print arXiv:2410.07854, 2024  
  <a href="https://arxiv.org/abs/2410.07854">[Paper]</a>
</i></p>
</li>
</ul>	

	

<ul> 
<li><p><a href="">SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks</a> <br />
Haiyang Wang, Qian Zhu, Mowen She, Yabo Li, Haoyu Song, Minghe Xu, <b>Xiao Wang</b> <br />
<i> arXiv Pre-print arXiv:2410.07857, 2024  
  <a href="https://arxiv.org/abs/2410.07857">[Paper]</a>
</i></p>
</li>
</ul>	


<ul> 
<li><p><a href="">Multi-modal Fusion based Q-distribution Prediction for Controlled Nuclear Fusion</a> <br />
Shiao Wang, Yifeng Wang, Qingchuan Ma, <b>Xiao Wang</b>, Ning Yan, Qingquan Yang, Guosheng Xu, Jin Tang <br />
<i> arXiv Pre-print arXiv:2410.08879, 2024  
  <a href="https://arxiv.org/abs/2410.08879">[Paper]</a>
</i></p>
</li>
</ul>



<ul> 
<li><p><a href="">Exploiting Memory-aware Q-distribution Prediction for Nuclear Fusion via Modern Hopfield Network</a> <br />
Qingchuan Ma, Shiao Wang, Tong Zheng, Xiaodong Dai, Yifeng Wang, Qingquan Yang, <b>Xiao Wang</b> <br />
<i> arXiv Pre-print arXiv:2410.08889, 2024  
  <a href="https://arxiv.org/abs/2410.08889">[Paper]</a>
</i></p>
</li>
</ul>
	

<ul> 
<li><p><a href="">Continuous-time Object Segmentation using High Temporal Resolution Event Camera</a> <br />
  Lin Zhu, Xianzhang Chen, Lizhi Wang, <b>Xiao Wang</b>, Yonghong Tian, and Hua Huang <br />
<i> IEEE Transactions on Pattern Analysis and Machine Intelligence 2024   
  <a href="https://ieeexplore.ieee.org/document/10713285">[Paper]</a>
  <a href="https://sites.google.com/view/ecos-net/">[Project Page]</a>
</i></p>
</li>
</ul>



<ul> 
<li><p><a href="">CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset</a> <br />
  <b>Xiao Wang</b>, Fuling Wang, Yuehang Li, Qingchuan Ma, Shiao Wang, Bo Jiang, Chuanfu Li, Jin Tang <br />
<i> arXiv Pre-print arXiv:2410.00379, 2024  
  <a href="https://arxiv.org/abs/2410.00379">[Paper]</a>
  <a href="https://github.com/Event-AHU/Medical_Image_Analysis">[Code]</a>
</i></p>
</li>
</ul>

	

<ul> 
<li><p><a href="">Temporal adaptive bidirectional bridging for RGB-D tracking</a> <br />
  Ge Ying, Dawei Zhang, Zhou Ou, <b>Xiao Wang</b>, Zhonglong Zheng <br />
<i> Pattern Recognition, 2024 
  <a href="https://www.sciencedirect.com/science/article/pii/S0031320324008045">[Paper]</a>
</i></p>
</li>
</ul>




<ul> 
<li><p><a href="">MambaEVT: Event Stream based Visual Object Tracking using State Space Model</a> <br />
  <b>Xiao Wang</b>, Chao Wang, Shiao Wang, Xixi Wang, Zhicheng Zhao, Lin Zhu, Bo Jiang <br />
<i> arXiv Pre-print arXiv:2408.10487, 2024 
  <a href="https://arxiv.org/abs/2408.10487">[arXiv]</a>
  <a href="https://github.com/Event-AHU/MambaEVT">[Code]</a>
</i></p>
</li>
</ul>

	
<ul> 
<li><p><a href="">Event Stream based Sign Language Translation: A High-Definition Benchmark Dataset and A New Algorithm</a> <br />
  <b>Xiao Wang</b>, Yao Rong, Fuling Wang, Jianing Li, Lin Zhu, Bo Jiang, Yaowei Wang <br />
<i> arXiv Pre-print arXiv:2408.10488, 2024 
  <a href="https://arxiv.org/abs/2408.10488">[arXiv]</a>
  <a href="https://github.com/Event-AHU/OpenESL">[Code]</a>
</i></p>
</li>
</ul>

	



<ul> 
<li><p><a href="">Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework</a> <br />
  Jiandong Jin, <b>Xiao Wang</b>, Qian Zhu, Haiyang Wang, Chenglong Li <br />
<i> arXiv Pre-print arXiv:2408.09720, 2024 
  <a href="https://arxiv.org/abs/2408.09720">[arXiv]</a>
  <a href="https://github.com/Event-AHU/OpenPAR">[Code]</a>
</i></p>
</li>
</ul>	

	
<ul> 
<li><p><a href="">R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation</a> <br />
  <b>Xiao Wang</b>, Yuehang Li, Fuling Wang, Shiao Wang, Chuanfu Li, Bo Jiang <br />
<i> arXiv Pre-print arXiv:2408.09743, 2024 
  <a href="https://arxiv.org/abs/2408.09743">[arXiv]</a>
  <a href="https://github.com/Event-AHU/Medical_Image_Analysis">[Code]</a>
</i></p>
</li>
</ul>	
	



<ul> 
<li><p><a href="">Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms</a> <br />
  <b>Xiao Wang</b>, Shiao Wang, Pengpeng Shao, Bo Jiang, Lin Zhu, Yonghong Tian <br />
<i> arXiv Pre-print arXiv:2408.09764, 2024 
  <a href="https://arxiv.org/abs/2408.09764">[arXiv]</a>
  <a href="https://github.com/Event-AHU/CeleX-HAR">[Code]</a>
</i></p>
</li>
</ul>	
	



<ul> 
<li><p><a href="">Treat Stillness with Movement: Remote Sensing Change Detection via Coarse-grained Temporal Foregrounds Mining</a> <br />
  Zitian Wang†, Xixi Wang†, Jingtao Jiang, Lan Chen*, <b>Xiao Wang</b>, Bo Jiang <br />
<i> arXiv Pre-print arXiv:2408.08078, 2024 
  <a href="https://arxiv.org/abs/2408.08078">[arXiv]</a>
  <a href="https://github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection">[Code]</a>
</i></p>
</li>
</ul>	
	


<ul> 
<li><p><a href="">An Empirical Study of Mamba-based Pedestrian Attribute Recognition</a> <br />
  <b>Xiao Wang</b>, Weizhe Kong, Jiandong Jin, Shiao Wang, Ruichong Gao, Qingchuan Ma, Chenglong Li, Jin Tang <br />
<i> arXiv Pre-print arXiv:2407.10374, 2024 
  <a href="https://arxiv.org/abs/2407.10374">[Paper]</a>
  <a href="https://github.com/Event-AHU/OpenPAR/tree/main/MambaPAR_Empirical_Study">[Code]</a>
</i></p>
</li>
</ul>	




<ul> 
<li><p><a href="">Learning Dynamic Batch-Graph Representation for Deep Representation Learning</a> <br />
  Xixi Wang, Bo Jiang*, <b>Xiao Wang</b>, Bin Luo <br />
<i> International Journal of Computer Vision (IJCV), 2024 
  <a href="https://link.springer.com/article/10.1007/s11263-024-02175-8">[Paper]</a>
  <a href="https://github.com/SissiW/DyBGR">[Code]</a>
</i></p>
</li>
</ul>	


<ul> 
<li><p><a href="">Temporal Residual Guided Diffusion Framework for Event-Driven Video Reconstruction</a> <br />
  Lin Zhu, Yunlong Zheng, Yijun Zhang, <b>Xiao Wang</b>, Lizhi Wang, Hua Huang <br />
<i> European Conference on Computer Vision (ECCV) 2024, <font color="#FF0000">Oral Representation</font> 
  <a href="https://arxiv.org/abs/2407.10636">[Paper]</a>
  <a href="">[Code]</a>
</i></p>
</li>
</ul>	


<ul> 
<li><p><a href="">Retain, Blend, and Exchange: A Quality-aware Spatial-Stereo Fusion Approach for Event Stream Recognition</a> <br />
  Lan Chen, Dong Li, <b>Xiao Wang*</b>, Pengpeng Shao, Wei Zhang, Yaowei Wang, Yonghong Tian, Jin Tang <br />
<i> In Peer Review, 2024 
  <a href="https://arxiv.org/abs/2406.18845">[arXiv]</a>
  <a href="https://github.com/Event-AHU/EFV_event_classification/tree/EFVpp">[Github]</a>
</i></p>
</li>
</ul>





<ul> 
<li><p><a href="">VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models</a> <br />
  Wentao Wu†, Fanghua Hong†, <b>Xiao Wang*</b>, Chenglong Li, Jin Tang <br />
<i> In Peer Review, 2024 
  <a href="https://arxiv.org/abs/2408.13031">[arXiv]</a>
  <a href="https://github.com/Event-AHU/VFM-Det">[Github]</a>
</i></p>
</li>
</ul>





<ul> 
<li><p><a href="">Mamba-FETrack: Frame-Event Tracking via State Space Model</a> <br />
  Ju Huang, Shiao Wang, Shuai Wang, Zhe Wu, <b>Xiao Wang*</b>, Bo Jiang <br />
<i> arXiv pre-print 2404.18174, PRCV-2024 
  <a href="https://arxiv.org/abs/2404.18174">[arXiv]</a>
  <a href="https://github.com/Event-AHU/Mamba_FETrack">[Github]</a>
  <a href="http://2024.prcv.cn/">[第七届中国模式识别与计算机视觉大会 (The 7th Chinese Conference on Pattern Recognition and Computer Vision PRCV 2024)]</a>
</i></p>
</li>
</ul>


  

<ul> 
<li><p><a href="">Pre-training on High Definition X-ray Images: An Experimental Study</a> <br />
  <b>Xiao Wang</b>, Yuehang Li, Wentao Wu, Jiandong Jin, Yao Rong, Bo Jiang, Chuanfu Li, Jin Tang <br />
<i> arXiv pre-print, arXiv, 2024 
  <a href="https://arxiv.org/abs/2404.17926">[arXiv]</a>
  <a href="https://github.com/Event-AHU/Medical_Image_Analysis">[Github]</a>
</i></p>
</li>
</ul>
 
 
<ul> 
<li><p><a href="">Spatio-Temporal Side Tuning Pre-trained Foundation Models for Video-based Pedestrian Attribute Recognition</a> <br />
  <b>Xiao Wang</b>, Qian Zhu, Jiandong Jin, Jun Zhu, Futian Wang, Bo Jiang, Yaowei Wang, Yonghong Tian <br />
<i> arXiv pre-print, arXiv, 2024 
  <a href="https://arxiv.org/abs/2404.17929">[arXiv]</a>
  <a href="https://github.com/Event-AHU/OpenPAR">[Github]</a>
</i></p>
</li>
</ul>



  

<ul> 
<li><p><a href="">MutualFormer: Multi-modal Representation Learning via Cross-Diffusion Attention</a> <br />
  Xixi Wang; Bo Jiang; <b>Xiao Wang</b>; Bin Luo <br />
  <i> International Journal of Computer Vision (IJCV), 2024 
  <a href="https://link.springer.com/article/10.1007/s11263-024-02067-x">[IJCV PDF]</a>
  <a href="https://arxiv.org/abs/2112.01177">[arXiv]</a>
  <a href="https://github.com/SissiW/MutualFormer">[Github]</a>
  <a href="https://www.ahu.edu.cn/2024/0508/c15129a339362/page.htm">[安徽大学新闻]</a>
</i></p>
</li>
</ul>


  

<ul> 
<li><p><a href="">Learning Graph Attentions via Replicator Dynamics</a> <br />
  Bo Jiang, Ziyan Zhang, Sheng Ge, Beibei Wang, <b>Xiao Wang</b>, Jin Tang <br />
  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2024
  <a href="https://ieeexplore.ieee.org/abstract/document/10508103/">[IEEE PDF]</a>
  <a href="https://www.ahu.edu.cn/2024/0508/c15129a339362/page.htm">[安徽大学新闻]</a>
</i></p>
</li>
</ul>
  
  
  


<ul> 
<li><p><a href="">State Space Model for New-Generation Network Alternative to Transformers: A Survey</a> <br />
 <b>Xiao Wang</b>, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bo Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian, Jin Tang <br />
<i>arXiv pre-print, arXiv:2404.09516, 2024, <font color="#FF0000">The First SSM/Mamba Survey</font>
  <a href="https://arxiv.org/abs/2404.09516">[arXiv]</a>
  <a href="https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List/blob/main/SSM_AHU_survey_v1.pdf">[PDF]</a>
  <a href="https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List">[Paper List]</a>
  <a href="https://mp.weixin.qq.com/s/ySA5-rP5XJdDJSnsQlt7FQ">[专知公众号]</a>
  <a href="https://zhuanlan.zhihu.com/p/693727335">[知乎]</a>
  <a href="https://hub.baai.ac.cn/paper/fef9e087-df00-49dc-aa01-7824394edeb4">[智源社区]</a>
</i></p>
</li>
</ul>


<ul> 
<li><p><a href="">Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline</a> <br />
 <b>Xiao Wang</b>, Ju Huang, Shiao Wang, Chuanming Tang, Bo Jiang, Yonghong Tian, Jin Tang, Bin Luo <br />
<i>arXiv preprint, arXiv:2403.05839 (2024).
  <a href="https://arxiv.org/abs/2403.05839">[arXiv]</a>
  <a href="https://github.com/Event-AHU/FELT_SOT_Benchmark">[Github]</a> 
  <a href="https://youtu.be/6zxiBHTqOhE?si=6ARRGFdBLSxyp3G8">[Youtube]</a> 
</i></p>
</li>
</ul>
  

<ul> 
<li><p><a href="">Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline</a> <br />
 <b>Xiao Wang</b>, Shiao Wang, Chuanming Tang, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang <br />
<i>CVPR-2024, arXiv 2309.14611
  <a href="https://arxiv.org/abs/2309.14611">[arXiv]</a>
  <a href="https://github.com/Event-AHU/EventVOT_Benchmark">[Github]</a> 
  <a href="https://youtu.be/FcwH7tkSXK0?si=GHOG7rfw4-GFd9dz">[Youtube]</a> 
  <a href="https://cvpr.thecvf.com/virtual/2024/poster/29490">[Video Tutorial]</a>
  <a href="https://cs.ahu.edu.cn/2024/0228/c11156a330018/page.htm">[AHU-CS-News]</a>
  <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Event_Stream-based_Visual_Object_Tracking_A_High-Resolution_Benchmark_Dataset_and_CVPR_2024_paper.html">[CVF-Paper]</a>  
</i></p>
</li>
</ul>

  


 
  

<ul> 
<li><p><a href="">Uncertainty-aware Bridge based Mobile-Former Network for Event-based Pattern Recognition</a> <br />
Haoxiang Yang, Chengguo Yuan, Yabin Zhu, Lan Chen, <b>Xiao Wang*</b>, Futian Wang <br />
ICSIPC 2024, China, https://doi.org/10.1117/12.3041069, 
<i>arXiv pre-print, arXiv:2401.11123
<a href="https://arxiv.org/abs/2401.11123">[Paper]</a> 
<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13253/1325312/Uncertainty-aware-bridge-based-mobile-former-network-for-event-based/10.1117/12.3041069.short?tab=ArticleLink">[Official]</a>
<a href="https://github.com/Event-AHU/Uncertainty_aware_MobileFormer">[GitHub]</a>
</i></p>
</li>
</ul> 

	


<ul> 
<li><p><a href="">CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras</a> <br />
  Yabin Zhu, <b>Xiao Wang*</b>, Chenglong Li, Bo Jiang, Lin Zhu, Zhixiang Huang, Yonghong Tian, Jin Tang <br />
<i>arXiv pre-print, arXiv:2401.02826
  <a href="https://arxiv.org/abs/2401.02826">[Paper]</a>
  <a href="https://github.com/Event-AHU/Cross_Resolution_SOT">[GitHub]</a>
  <a href="https://github.com/Event-AHU/Cross_Resolution_SOT#dvd-crsot-dataset-download">[Dataset Download]</a>
</i></p>
</li>
</ul> 


  




<ul> 
<li><p><a href="">Pedestrian Attribute Recognition via CLIP based Prompt Vision-Language Fusion</a> <br />
<b>Xiao Wang</b>, Jiandong Jin, Chenglong Li, Jin Tang, Cheng Zhang, Wei Wang <br />
<i>arXiv pre-print, arXiv:2312.10692, IEEE TCSVT 2024. 
  <a href="https://arxiv.org/abs/2312.10692">[Paper]</a>
  <a href="https://github.com/Event-AHU/OpenPAR">[Code]</a>
</i></p>
</li>
</ul> 

  



<ul> 
<li><p><a href="">Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition</a> <br />
<b>Xiao Wang</b>, Yao Rong, Shiao Wang, Yuan Chen, Zhe Wu, Bo Jiang, Yonghong Tian, Jin Tang <br />
<i>arXiv pre-print, arXiv:2312.11128
  <a href="https://arxiv.org/abs/2312.11128">[Paper]</a>
  <a href="https://github.com/Event-AHU/TSCFormer">[Code]</a>
</i></p>
</li>
</ul> 

  


<ul> 
<li><p><a href="">Finding Visual Saliency in Continuous Spike Stream</a> <br />
Lin Zhu, Xianzhang Chen, <b>Xiao Wang</b>, Hua Huang <br />
<i>AAAI-2024, arXiv pre-print, arXiv:2403.06233 
  <a href="https://arxiv.org/abs/2403.06233">[Paper]</a>
  <a href="https://github.com/BIT-Vision/SVS">[GitHub]</a>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28610">[AAAI-Paper]</a>
  <a href="https://underline.io/lecture/93075-finding-visual-saliency-in-continuous-spike-stream">[Video]</a>
</i></p>
</li>
</ul>  





  
<ul> 
<li><p><a href="">Structural Information Guided Multimodal Pre-training for Vehicle-centric Perception</a> <br />
<b>Xiao Wang</b>, Wentao Wu, Chenglong Li, Zhicheng Zhao, Zhe Chen, Yukai Shi, Jin Tang <br />
<i>AAAI-2024, arXiv pre-print, arXiv:2312.09812 
  <a href="https://arxiv.org/abs/2312.09812">[arXiv]</a>
  <a href="https://github.com/Event-AHU/VehicleMAE">[Github]</a>
  <a href="https://ai.ahu.edu.cn/2023/1226/c19133a327503/page.htm">[AHU-News]</a>
  <a href="https://github.com/wangxiao5791509/wangxiao5791509.github.io/tree/main/%E5%AD%A6%E7%94%9F%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/AAAI_2024">[Photos From AAAI-2024]</a>
</i></p>
</li>
</ul>  


  

<ul> 
<li><p><a href="">HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors</a> <br />
 <b>Xiao Wang</b>, Zongzhen Wu, Bo Jiang, Zhimin Bao, Lin Zhu, Guoqi Li, Yaowei Wang, Yonghong Tian <br />
<i>AAAI-2024, arXiv pre-print, arXiv:2211.09648 
  <a href="https://arxiv.org/abs/2211.09648">[arXiv]</a>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28372">[AAAI2024]</a>
  <a href="https://github.com/Event-AHU/HARDVS">[GitHub]</a>
  <a href="https://youtu.be/AgYjh-pfUT0">[DemoVideo]</a>
  <a href="https://sites.google.com/view/hardvs/">[Project]</a>
  <a href="https://github.com/wangxiao5791509/wangxiao5791509.github.io/tree/main/%E5%AD%A6%E7%94%9F%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/AAAI_2024">[Photos From AAAI-2024]</a>
</i></p>
</li>
</ul>

  

<ul> 
<li><p><a href="">SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm</a> <br />
Jiandong Jin, <b>Xiao Wang*</b>, Chenglong Li, Lili Huang, Jin Tang <br />
<i>arXiv:2312.01640, 2023
  <a href="https://arxiv.org/abs/2312.01640">[arXiv]</a>
  <a href="https://github.com/Event-AHU/OpenPAR">[Github]</a> 
</i></p>
</li>
</ul>  



  

<ul> 
<li><p><a href="">Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models</a> <br />
Dong Li, Jiandong Jin, Yuhao Zhang, Yanlin Zhong, Yaoyang Wu, Lan Chen*, <b>Xiao Wang*</b>, Bin Luo <br />
<i>arXiv:2311.18592, Pattern Recognition 2024 
  <a href="https://arxiv.org/abs/2311.18592">[arXiv]</a>
  <a href="https://www.sciencedirect.com/science/article/pii/S0031320324008318">[PR Paper]</a>
  <a href="https://github.com/Event-AHU/SAFE_LargeVLM">[Github]</a> 
</i></p>
</li>
</ul>  


<ul> 
<li><p><a href="">VcT: Visual change Transformer for Remote Sensing Image Change Detection</a> <br />
Bo Jiang, Zitian Wang, Xixi Wang, Ziyan Zhang, Lan Chen, <b>Xiao Wang*</b>, Bin Luo* <br />
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS) 2023  
  <a href="https://arxiv.org/abs/2310.11417">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/abstract/document/10294300">[IEEE]</a> 
  <a href="https://github.com/Event-AHU/VcT_Remote_Sensing_Change_Detection">[Github]</a> 
</i></p>
</li>
</ul>  

  
  



  

<ul> 
<li><p><a href="">Unified and Dynamic Graph for Temporal Character Grouping in Long Videos</a> <br />
 Xiujun Shu, Wei Wen, Liangsheng Xu, Mingbao Lin, Ruizhi Qiao, Taian Guo, Hanjun Li, Bei Gan, <b>Xiao Wang</b>, Xin Sun <br />
<i>arXiv 2308.14105  
  <a href="https://arxiv.org/pdf/2308.14105.pdf">[arXiv]</a>
</i></p>
</li>
</ul>

  

<ul> 
<li><p><a href="">Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification</a> <br />
 Chengguo Yuan, Yu Jin, Zongzhen Wu, Fanting Wei, Yangzirui Wang, Lan Chen, and <b>Xiao Wang</b> <br />
<i> Pattern Recognition and Computer Vision (PRCV)-2013   
  <a href="https://arxiv.org/abs/2308.11937">[arXiv]</a>
  <a href="https://github.com/Event-AHU/EFV_event_classification">[Github]</a>
  <a href="https://link.springer.com/chapter/10.1007/978-981-99-8429-9_1">[PRCV]</a>
</i></p>
</li>
</ul>





<ul> 
<li><p><a href="">SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition</a> <br />
 <b>Xiao Wang</b>, Zongzhen Wu, Yao Rong, Lin Zhu, Bo Jiang, Jin Tang, Yonghong Tian <br />
<i>arXiv:2308.04369, 2023  
  <a href="https://arxiv.org/abs/2308.04369">[arXiv]</a>
  <a href="https://github.com/Event-AHU/SSTFormer">[Github]</a>
</i></p>
</li>
</ul>

  
<ul> 
<li><p><a href="">Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition</a> <br />
 Bo Jiang, Chengguo Yuan, <b>Xiao Wang*</b>, Zhimin Bao, Lin Zhu, Yonghong Tian, Jin Tang <br />
<i>arXiv:2306.05239, 2023  
  <a href="https://arxiv.org/abs/2306.05239">[arXiv]</a>
  <a href="https://github.com/Event-AHU/AGCN_Event_Classification">[Github]</a>
</i></p>
</li>
</ul>
  
  
<ul> 
<li><p><a href="">Deep Triply Attention Network for RGBT Tracking</a> <br />
 Rui Yang, <b>Xiao Wang</b>, Yabin Zhu, Jin Tang <br />
<i>Cognitive Computation 2023  
  <a href="https://link.springer.com/article/10.1007/s12559-023-10158-z">[Paper]</a>
</i></p>
</li>
</ul> 
  
  
<ul> 
<li><p><a href="">AMatFormer: Efficient Feature Matching via Anchor Matching Transformer</a> <br />
 Bo Jiang, Shuxian Luo, <b>Xiao Wang*</b>, Chuanfu Li and Jin Tang <br />
<i>IEEE Transactions on Multimedia (TMM) 2023  
  <a href="https://arxiv.org/abs/2305.19205">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10143284/">[IEEE]</a>
</i></p>
</li>
</ul> 

  
  
<ul> 
<li><p><a href="">Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition</a> <br />
 Jun Zhu†, Jiandong Jin†, Zihan Yang, Xiaohao Wu, <b>Xiao Wang*</b> († denotes equal contribution) <br />
<i>CVPR-2023 Workshop@NFVLR (New Frontiers in Visual Language Reasoning: Compositionality, Prompts and Causality) 
  <a href="https://arxiv.org/abs/2304.10091">[arXiv]</a>
  <a href="https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/papers/Zhu_Learning_CLIP_Guided_Visual-Text_Fusion_Transformer_for_Video-Based_Pedestrian_Attribute_CVPRW_2023_paper.pdf">[CVF]</a>
  <a href="https://github.com/Event-AHU/VTF_PAR">[Github]</a>
</i></p>
</li>
</ul> 


  
<ul> 
<li><p><a href="">RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided Learning</a> <br />
 Yabin Zhu, Chenglong Li, <b>Xiao Wang</b>, Jin Tang, and Zhixiang Huang <br />
<i>arXiv:2303.14778, IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2024 
  <a href="https://arxiv.org/pdf/2303.14778.pdf">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10506555">[IEEE]</a>
</i></p>
</li>
</ul> 
  
  


  
<ul> 
<li><p><a href="">Transformer Vision-Language Tracking via Proxy Token Guided Cross-Modal Fusion</a> <br />
 Haojie Zhao, <b>Xiao Wang</b>, Dong Wang, Huchuan Lu, and Xiang Ruan <br />
<i>Pattern Recognition Letters (PRL), Elsevier, 2023 
  <a href="">[arXiv]</a>
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865523000545">[PRL]</a>
  <a href="">[GitHub]</a>
</i></p>
</li>
</ul> 
  
  

  
<ul> 
<li><p><a href="">Rethinking Batch Sample Relationships for Data Representation: A Batch-Graph Transformer based Approach</a> <br />
 Xixi Wang, Bo Jiang, <b>Xiao Wang</b>, Bin Luo <br />
<i>IEEE Transactions on Multimedia (TMM) 2023, 
  <a href="https://arxiv.org/abs/2211.10622">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10144585">[IEEE]</a>
</i></p>
</li>
</ul>    
 
  
  
<ul> 
<li><p><a href="">Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric</a> <br />
 Chuanming Tang, <b>Xiao Wang*</b>, Ju Huang, Bo Jiang, Lin Zhu, Jianlin Zhang*, Yaowei Wang, Yonghong Tian <br />
<i>arXiv pre-print, arXiv:2211.11010 
  <a href="https://arxiv.org/abs/2211.11010">[arXiv]</a>
  <a href="https://github.com/Event-AHU/COESOT">[GitHub]</a>
  <a href="https://youtu.be/_ROv09rvi2k">[DemoVideo]</a>
  <a href="https://sites.google.com/view/coesot/">[Project]</a>
</i></p>
</li>
</ul>  


  
  


  
  



<ul> 
<li><p><a href="">Few-Shot Learning Meets Transformer: Unified Query-Support Transformers for Few-Shot Classification</a> <br />
  Xixi Wang, <b>Xiao Wang</b>, Bo Jiang, Bin Luo <br />
<i> IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2023 
  <a href="https://arxiv.org/abs/2208.12398">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/abstract/document/10144072/">[IEEE]</a>
  <a href="https://github.com/SissiW/QSFormer">[GitHub]</a>
</i></p>
</li>
</ul>

  


  
<ul> 
<li><p><a href="">Learning Spatial-Frequency Transformer for Visual Object Tracking</a> <br />
  Chuanming Tang, <b>Xiao Wang*</b>, Yuanchao Bai, Zhe Wu, Jianlin Zhang, Yongmei Huang <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023  
  <a href="https://arxiv.org/abs/2208.08829">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10054166">[IEEE]</a>
  <a href="https://github.com/Tchuanm/SFTransT">[Github]</a> 
</i></p>
</li>
</ul>

  

<ul> 
<li><p><a href="">Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey</a> <br />
 <b>Xiao Wang</b>, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, Wen Gao <br />
<i>Machine Intelligence Research (MIR), 2022 
  <a href="https://arxiv.org/abs/2302.10035">[arXiv]</a>
  <a href="https://www.mi-research.net/article/doi/10.1007/s11633-022-1410-8">[MIR]</a> 
  <a href="https://mp.weixin.qq.com/s/5eELXfACI67yZT7WUtMFMA">[极市平台公众号]</a> 
  <a href="https://youtu.be/zQxV-SUz6zU?si=e27cyVjMUdU-XEwd">[Machine Intelligence Research (Youtube)]</a> 
  <a href="https://mp.weixin.qq.com/s/yX1DdDCA-nMluzOB6Qz3sw">[ MIR编辑部@机器智能研究MIR]</a> 
  <a href="https://github.com/wangxiao5791509/MultiModal_BigModels_Survey">[Github]</a> 
  <a href="https://mp.weixin.qq.com/s/R9uZqe2ZByYHziTp0nZIIA">[<font color="#FF0000">MIR Download Count TOP10 Articles (First Place, 6618, 2024.06.20)</font>]</a>
</i></p>
</li>
</ul> 



 


  

<ul>
<li><p><a href="">See Finer, See More: Implicit Modality Alignment for Text-based Person Retrieval</a> <br />
Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song, Ruizhi Qiao, Bo Ren, <b>Xiao Wang*</b> <br />
<i>The 2nd Workshop on Real-World Surveillance: Applications and Challenges, ECCVW-2022
  <a href="https://arxiv.org/abs/2208.08608">[arXiv]</a>
  <a href="https://vap.aau.dk/rws-eccv2022/">[ECCV]</a>
  <a href="https://youtu.be/h8zCTLjJAxM">[YouTube]</a>
  <a href="https://github.com/shuxjweb/IVT">[Code]</a>
</i></p>
</li>
</ul>
  
  





<ul>
<li><p><a href="https://arxiv.org/abs/2207.12767">Criteria Comparative Learning for Real-scene Image Super-Resolution</a> <br />
Yukai Shi, Hao Li, Sen Zhang, Zhijing Yang, <b>Xiao Wang</b> <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology, 2022 
  <a href="https://arxiv.org/abs/2207.12767">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9847265">[IEEE]</a>
  <a href="https://github.com/House-Leo/RealSR-CCL">[Code]</a> 
  <a href="https://github.com/House-Leo/RealSR-Zero">[Dataset]</a> 
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2205.13125">Prompt-based Learning for Unpaired Image Captioning</a> <br />
Peipei Zhu, <b>Xiao Wang</b>, Lin Zhu, Zhenglong Sun, Weishi Zheng, Yaowei Wang, Changwen Chen <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2205.13125">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10097833">[IEEE]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2107.10433">MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking</a> <br />
<b>Xiao Wang</b>, Xiujun Shu, Shiliang Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2107.10433">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9772993">[IEEE]</a>
  <a href="https://github.com/wangxiao5791509/MFG_RGBT_Tracking_PyTorch">[Code]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2203.03195">Unpaired Image Captioning by Image-level Weakly-Supervised Visual Concept Recognition</a> <br />
Peipei Zhu, <b>Xiao Wang</b>, Yong Luo, Zhenglong Sun, Wei-Shi Zheng, Yaowei Wang, Changwen Chen <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2203.03195">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9917391">[IEEE]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/abs/2202.05659">Tiny Object Tracking: A Large-scale Dataset and A Baseline</a> <br />
Yabin Zhu, Chenglong Li, Yao Liu, <b>Xiao Wang</b>, Jin Tang, Bin Luo, Zhixiang Huang <br />
<i>IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS), 2023. 
<a href="https://arxiv.org/abs/2202.05659">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/10035907">[IEEE]</a>
<a href="https://github.com/ZYB0726/MKDNet">[Dataset and Evaluation Toolkit]</a> 
</i></p>
</li>
</ul>

  
  

  
<ul>
<li><p><a href="https://arxiv.org/pdf/2201.10943.pdf">Event-based Video Reconstruction via Potential-assisted Spiking Neural Network</a> <br />
Lin Zhu, <b>Xiao Wang</b>, Yi Chang, Jianing Li, Tiejun Huang, Yonghong Tian <br />
<i>CVPR, 2022. <a href="https://arxiv.org/pdf/2201.10943.pdf">[arXiv]</a>
  <a href="https://github.com/LinZhu111/EVSNN">[Code]</a>
</i></p>
</li>
</ul>

  
   
    
  
  

  
<ul>
<li><p><a href="https://www.aaai.org/AAAI22Papers/AAAI-1396.LiJ.pdf">Retinomorphic Object Detection in Asynchronous Visual Streams</a> <br />
Jianing Li+, <b>Xiao Wang+</b>, Lin Zhu, Jia Li, Tiejun Huang, Yonghong Tian (+ denotes equal contribution) <br />
<i>AAAI, 2022, <font color="#FF0000">Oral representation</font>. 
<a href="https://www.aaai.org/AAAI22Papers/AAAI-1396.LiJ.pdf">[PDF]</a>
<a href="https://aaai-2022.virtualchair.net/poster_aaai1396">[Poster]</a>
<a href="https://www.pkuml.org/resources/pku-vidar-dvs.html">[Project Page]</a>
<a href="https://git.openi.org.cn/lijianing/PKU-Vidar-DVS/datasets">[Dataset]</a>
</i></p>
</li>
</ul>
  
  
  
   
  
<ul>
<li><p><a href="https://arxiv.org/abs/2108.05015">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows</a> <br />
<b>Xiao Wang</b>, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Cybernetics, 2023. 
<a href="https://arxiv.org/abs/2108.05015">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/10284004">[IEEE]</a>
<a href="https://sites.google.com/view/viseventtrack/">[Project]</a>
<a href="https://www.youtube.com/watch?v=U4uUjci9Gjc&ab_channel=XiaoWang">[Demo]</a>
<a href="https://www.youtube.com/watch?v=vGwHI2d2AX0&ab_channel=XiaoWang">[Video Tutorial]</a>
<a href="https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark">[Dataset and Evaluation Toolkit]</a>
<a href="https://github.com/wangxiao5791509/RGB-DVS-SOT-Baselines">[Source Code]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2205.09676">Beyond Greedy Search: Tracking by Multi-Agent Reinforcement Learning-based Beam Search</a> <br />
<b>Xiao Wang</b>, Zhe Chen, Bo Jiang, Jin Tang, Bin Luo, Dacheng Tao <br />
<i>IEEE Transactions on Image Processing (TIP), 2022. 
<a href="https://arxiv.org/abs/2205.09676">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/9904473">[IEEE]</a>
<a href="https://sites.google.com/view/beamtracking/">[Project]</a>
<a href="https://www.youtube.com/watch?v=f1yiYv-SJyY&ab_channel=XiaoWang">[Video]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2205.09676">RGBT Tracking via Cross-modality Message Passing</a> <br />
Rui Yang, <b>Xiao Wang</b>, Chenglong Li, Jinmin Hu, Jin Tang <br />
<i>Neurocomputing, 2021. 
<a href="https://www.sciencedirect.com/science/article/pii/S0925231221011966">[PDF]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/abs/2108.00803">Learn to Match: Automatic Matching Network Design for Visual Tracking</a> <br />
Zhipeng Zhang, Yihao Liu, <b>Xiao Wang</b>, Bing Li, Weiming Hu <br />
<i>ICCV, 2021. 
<a href="https://arxiv.org/abs/2108.00803">[arXiv]</a>
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learn_To_Match_Automatic_Matching_Network_Design_for_Visual_Tracking_ICCV_2021_paper.pdf">[ICCV]</a>
<a href="https://github.com/JudasDie/SOTS">[Code]</a>
<a href="https://www.youtube.com/watch?v=iABDcmi3gVI&ab_channel=XiaoWang">[Video]</a>
<a href="https://drive.google.com/file/d/1CcBuGyivdSmvp8dqtrt4kttQEsrkQETG/view">[Poster]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf">NeuSpike-Net: High Speed Image Reconstruction via Bio-inspired Neuromorphic Cameras</a> <br />
Lin Zhu, Jianing Li, <b>Xiao Wang</b>, Tiejun Huang, Yonghong Tian <br />
<i>ICCV, 2021. 
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf">[PDF]</a>
<a href="https://sites.google.com/view/retina-recon">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9463711">Semantic-guided Pixel Sampling for Cloth-Changing Person Re-identification</a> <br />
Xiujun Shu, Ge Li, <b>Xiao Wang</b>, Weijian Ruan, Qi Tian <br />
<i>IEEE Signal Processing Letters, 2021. 
<a href="https://ieeexplore.ieee.org/document/9463711">[IEEE]</a>
<a href="https://arxiv.org/pdf/2107.11522.pdf">[arXiv]</a>
<a href="https://github.com/shuxjweb/pixel_sampling">[Code]</a>
</i></p>
</li>
</ul> 

  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9615085">Large-Scale Spatio-Temporal Person Re-identification: Algorithms and Benchmark</a> <br />
Xiujun Shu+, <b>Xiao Wang+</b>, Shiliang Zhang, Xianghao Zhang, Yuanqi Chen, Ge Li, Qi Tian (+ denotes equal contribution) <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021. 
<a href="https://ieeexplore.ieee.org/abstract/document/9615085">[IEEE]</a>
<a href="https://arxiv.org/pdf/2105.15076.pdf">[arXiv]</a>
<a href="https://sites.google.com/view/personreid">[Project]</a>
<a href="https://github.com/shuxjweb/last">[Code]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2103.16746">Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark</a> <br />
<b>Xiao Wang+</b>, Xiujun Shu+, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu (+ denotes equal contribution) <br />
<i>CVPR, 2021. 
<a href="https://arxiv.org/abs/2103.16746">[arXiv]</a>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Towards_More_Flexible_and_Accurate_Object_Tracking_With_Natural_Language_CVPR_2021_paper.pdf">[CVF]</a>
<a href="https://sites.google.com/view/langtrackbenchmark/">[Project]</a>
<a href="https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit">[EvaluationToolkit]</a>
<a href="https://www.youtube.com/watch?v=7lvVDlkkff0&ab_channel=XiaoWang">[Demo]</a>
<a href="https://www.youtube.com/watch?v=uE2qnNf-ClI&ab_channel=XiaoWang">[Tutorial]</a>
</i></p>
</li>
</ul>
  


<ul>
<li><p><a href="https://arxiv.org/abs/2106.04840">Tracking by Joint Local and Global Search: A Target-aware Attention based Approach</a> <br />
<b>Xiao Wang</b>, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, and Feng Wu <br />
<i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021. 
<a href="https://arxiv.org/abs/2106.04840">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/9511641">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/LGSearch_DDGAN_PyTorch">[Code]</a>
<a href="https://www.youtube.com/watch?v=uu00QIL7tjo&ab_channel=XiaoWang">[Tutorial]</a>
</i></p>
</li>
</ul>
  
  

<ul>
<li><p><a href="https://arxiv.org/pdf/2007.03584.pdf">STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification</a> <br />
Bo Jiang, Sheng Wang, <b>Xiao Wang</b>*, Aihua Zheng (* denotes corresponding author) <br />
<i>arxiv preprint arXiv:2007.03584, 2021. 
<a href="https://arxiv.org/pdf/2007.03584.pdf">[arXiv]</a>
<a href="https://github.com/wangxiao5791509/STADB_ReID">[Code]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.10280.pdf">cmSalGAN: RGB-D Salient Object Detection with Cross-View Generative Adversarial Networks</a> <br />
Bo Jiang, Zitai Zhou, <b>Xiao Wang*</b>, Jin Tang, Bin Luo (* denotes corresponding author) <br />
<i>IEEE Transactions on Multimedia (TMM), 2020. 
<a href="https://arxiv.org/pdf/1912.10280.pdf">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/9103135">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/cmSalGAN_PyTorch">[Code]</a>
<a href="https://sites.google.com/view/cmsalgan/">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9345930/">Dynamic Attention-guided Multi-Trajectory Analysis for Single Object Tracking</a> <br />
<b>Xiao Wang</b>, Zhe Chen, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021. 
<a href="https://arxiv.org/abs/2103.16086">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/9345930/">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/DeepMTA_PyTorch">[Code]</a>
<a href="https://sites.google.com/view/mt-track/home/">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/1811.10014">Describe and Attend to Track: Learning Natural Language guided Structural Representation and Visual Attention for Object Tracking</a> <br />
<b>Xiao Wang</b>, Chenglong Li, Rui Yang, Tianzhu Zhang, Jin Tang, Bin Luo <br />
<i>arxiv, preprint, arXiv:1811.10014, 2018. 
<a href="https://arxiv.org/abs/1811.10014">[arXiv]</a>
<a href="https://sites.google.com/view/languagetracking/">[Project]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0923596518308683">Quality–aware dual–modal saliency detection via deep reinforcement learning</a> <br />
<b>Xiao Wang</b>, Tao Sun, Rui Yang, Chenglong Li, Bin Luo, Jin Tang <br />
<i>Signal Processing and Image Communication, 2019. 
<a href="https://www.sciencedirect.com/science/article/pii/S0923596518308683">[PDF]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://bmvc2019.org/wp-content/uploads/papers/0562-paper.pdf">Learning Target-aware Attention for Robust Tracking with Conditional Adversarial Network</a> <br />
<b>Xiao Wang</b>, Tao Sun,  Rui Yang, Bin Luo  <br />
<i>30TH British Machine Vision Conference (BMVC), 2019. 
<a href="https://bmvc2019.org/wp-content/uploads/papers/0562-paper.pdf">[PDF]</a>
<a href="https://bmvc2019.org/wp-content/uploads/papers/0562-supplementary.pdf">[Supplementary]</a>
<a href="https://sites.google.com/view/globalattentiontracking">[Project]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0031320321004015">Pedestrian Attribute Recognition: A Survey</a> <br />
<b>Xiao Wang</b>, Shaofei Zheng, Rui Yang, Aihua Zheng, Zhe Chen, Bin Luo, Jin Tang  <br />
<i>Pattern Recognition, 2021. 
<a href="https://arxiv.org/abs/1901.07474">[arXiv]</a>
<a href="https://www.sciencedirect.com/science/article/pii/S0031320321004015">[Pattern Recognition]</a>
<a href="https://sites.google.com/view/ahu-pedestrianattributes/">[Project]</a>
<a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List">[Github]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/pdf/1908.04441.pdf">Learning Target-oriented Dual Attention for Robust RGB-T Tracking</a> <br />
Rui Yang, Yabin Zhu, <b>Xiao Wang</b>, Chenglong Li, Jin Tang  <br />
<i>IEEE International Conference on Image Processing (ICIP), 2019. 
<a href="https://arxiv.org/pdf/1908.04441.pdf">[arXiv]</a>
</i></p>
</li>
</ul>

  

<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S092523121931255X">Multi-modal foreground detection via inter- and intra-modality-consistent low-rank separation</a> <br />
Aihua Zheng,  Naipeng Ye, Chenglong Li, <b>Xiao Wang</b>, Jin Tang <br />
<i>Neurocomputing, 2020. 
<a href="https://www.sciencedirect.com/science/article/pii/S092523121931255X">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="Dense Feature Aggregation and Pruning for RGBT Tracking">Dense Feature Aggregation and Pruning for RGBT Tracking</a> <br />
Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, <b>Xiao Wang</b>  <br />
<i>Proceedings of the 27th ACM International Conference on Multimedia (ACM MM), 2019
<a href="https://arxiv.org/abs/1907.10451">[arXiv]</a>
<a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350928?casa_token=X-2D-Vf5XNIAAAAA:FP95TAXyz0gMYafxBEwbLCxG-fVXNFN8MC2TUPx1BxQsiFD4CgArbxObOQcSa-WA4MIAeWHAe8cKktk">[ACM MM]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation</a> <br />
<b>Xiao Wang</b>, Chenglong Li, Bin Luo, Jin Tang <br />
<i>CVPR, 2018
<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">[PDF]</a>
<a href="https://sites.google.com/view/cvpr2018sintplusplus/">[Project]</a>
</i></p>
</li>
</ul>
  


<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/7457366">Weighted Low-Rank Decomposition for Robust Grayscale-Thermal Foreground Detection</a> <br />
Chenglong Li+, <b>Xiao Wang+</b>, Lei Zhang, Jin Tang, Hejun Wu, Liang Lin (+ denotes equal contribution) <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017
<a href="https://ieeexplore.ieee.org/document/7457366">[PDF]</a>
<a href="https://sites.google.com/view/mmmovingobjectdetectiontip/">[Project]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7937922">Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning</a> <br />
Ziliang Chen, Keze Wang, <b>Xiao Wang</b>, Pai Peng, Ebroul Izquierdo, Liang Lin  <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017
<a href="https://ieeexplore.ieee.org/abstract/document/7937922">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7822984/">Grayscale-Thermal Object Tracking via Multitask Laplacian Sparse Representation</a> <br />
Li, Chenglong and Sun, Xiang and <b>Xiao Wang</b> and Zhang, Lei and Tang, Jin <br />
<i>IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2017
<a href="https://ieeexplore.ieee.org/abstract/document/7822984/">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
  
<h2>Class Schedule</h2>
<ul>
<li><p>2022-2023 秋学期    （本）MATLAB 编程基础 <br />
<li><p>2022-2023 秋学期    （本）数据挖掘与知识发现 <br />
<li><p>2022-2023 春学期    （本）机器学习 <br /> 
<li><p>2022-2023 春学期    （本）计算机伦理 <br /> 
<li><p>2023-2024 秋学期    （本）深度学习（理论及实验课） <a href="https://www.youtube.com/playlist?list=PLGx-IBAGRM8cEnXx9YOPG26FP2u_U42SU">[YouTube]</a> <br /> 
<li><p>2023-2024 秋学期    （本）计算机伦理 <br /> 
<li><p>2024-2025 春学期    （本）计算机导论 <br /> 
<li><p>2024-2025 春学期    （本）机器学习（理论及实验课） <br /> 
<li><p>2024-2025 春学期    （研）模式识别 <br /> 
</ul>

  
  
  
  
<h2>Video Tutorial and Others</h2>
<ul>
<li><p>浅谈科研论文写作 (2023-01-09) <a href="https://youtu.be/GK0Pq0fSe0M">[YouTube]</a> <br /> 
<li><p>状态空间模型：模型分析及下游应用 (2024-05-19) <a href="https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List/blob/main/SSM_Slides_2024.05.19.pdf">[Slides]</a> <br />  
<li><p>多模态视觉目标跟踪 (Multi-Modal Visual Object Tracking) (2024-06-12) 
	<a href="https://github.com/Event-AHU/MultiModal_VOT_Survey/blob/main/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A7%86%E8%A7%89%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA-AHU2024.06.12.pdf">[Slides]</a> 
	<a href="https://youtu.be/bW7ZDWOOp1M?si=-GuJE_y1yibtxg12">[YouTube]</a> <br /> 
<li><p><font color="#FF0000">Interview Video from CGTN (26-Aug-2020, Shenzhen uses AI for epidemic prevention)</font>
	<a href="https://news.cgtn.com/news/2020-08-26/Shenzhen-uses-AI-for-epidemic-prevention-Th1dbDLHkA/index.html?from=groupmessage">[News from CGTN]</a> 
	<a href="https://youtu.be/Tp9JKWiREWU?si=9cWg8891OyBRrS-j">[YouTube]</a><br /> 
<li><p>VALSE Webinar 24-20期 总第355期 面向事件相机的物体检测与跟踪 (2024-07-17) 
	<a href="https://www.bilibili.com/video/BV1w1421t7ya/?vd_source=309943935ab693366d5e9335381852f1">[B站视频回放]</a> 
	<a href="">[Slides]</a> 
	<a href="https://mp.weixin.qq.com/s/Sxp_a68b4uQsI9tJXb8-aQ">[News]</a> <br /> 
<li><p>CSIG云讲堂：大模型时代的行人属性识别 [Pedestrian Attribute Recognition in the Big Model Era] (2024.08.27)
	<a href="">[Video (To be Updated)]</a> 
	<a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List/blob/master/PAR-CSIG-2024.08.27.pdf">[Slides]</a> 
	<a href="https://mp.weixin.qq.com/s/EdnLEdu96Ne8LfCsMO7abQ">[News]</a> <br /> 	
</ul>


	





  
  

<h2>Awards</h2>
<ul>
<li><p>2014-2015    National Academic Scholarships <br />
<li><p>2015-2016    National Academic Scholarships <br />
<li><p>2016-2017    National Academic Scholarships <br />
<li><p>2017-2018    National Academic Scholarships <br />
<li><p>2018-2019    National Academic Scholarships <br />
<li><p>2018-2019    GuoYuan Scholarships <br />
<li><p>2019		安徽省校级优秀毕业生 AHU-University graduates <br /> 
<li><p>2019		<font color="#FF0000">安徽省省级优秀毕业生 AnHui Outstanding Graduates </font> <br /> 
<li><p>2022		<font color="#FF0000">安徽省合肥市D类高层次人才(市级领军人才)</font> <br /> 
<li><p>2023		<font color="#FF0000">ACM-合肥分会-“新星奖” (ACM CHINA COUNCIL  HEFEI CHAPTER RISING STAR, No.2023ACMCHINA-XX-C1002)</font> <br />
<li><p>2024		<font color="#FF0000">安徽省-省优青（科技厅）</font> <br /> 
</ul>
  
  



	
  
<h2>Academic Activities</h2>
<ul>  
<li><p>2022.05.22    IEEE Member (Member number: 96976176)
<li><p>2022.05.30    CSIG Member (Member number: E654403548M) 
<li><p>2024.08.06    Serving as an Associate Editor for the journal “IEEE Sensors Journal” (Impact Factor 4.3, JCR Q1, SCI Q2).
<li><p>2024.08.20    Serving as an Area Chair of ICLR (International Conference on Learning Representations) 2025. 
</ul>



<h2>Funds and Projects</h2>
<ul>
<li><p>博士后创新人才支持计划（“博新计划”）Postdoctoral Innovative Talent Support Program (BX20200174, 2021-2022) <br />
<li><p>中国博士后面上项目 China Postdoctoral Science Foundation Funded Project (2020M682828, 2021-2022) <br />
<li><p>国家自然科学基金（青年项目） National Natural Science Foundation of China (编号：62102205, 名称：面向神经形态视觉的目标跟踪关键技术研究, 2022-2024) <br />
<li><p>变电站智能巡检数据标注及检测模型研究 (2023.09-2025.09) <br />
<li><p>安徽省自然科学基金-优青项目（高效率低延迟低功耗事件相机目标跟踪）Anhui Provincial Natural Science Foundation - Outstanding Youth Project, 2408085Y032 (2024.09 - 2027.09) <br />
</ul>

	

<h2>Patents</h2>
<ul>  
<li><p> 一种基于空间-立体融合的事件流区分识别方法, CN118172706A, 王逍; 李冬; 江波; 2024.06.11 
<li><p> 基于结构信息引导以车为中心的多模态预训练系统及方法，CN117475278A，李成龙; 吴文滔; 王逍; 王伟; 章程; 汤进, 2024.01.30 
<li><p> 一种事件识别方法、系统、设备及介质, CN116740605A, 王逍; 袁程果; 江波,  2023.09.12
<li><p> 事件识别模型的训练方法、装置、设备及事件识别方法, CN116434122A, 王逍; 吴宗振; 江波, 2023.07.14 
<li><p> 基于提示微调预训练大模型的行人属性识别方法, CN116259075A, 李成龙; 金建东; 王逍; 汤进; 章程, 2023.06.13 
<li><p> 一种结合上下文信息的跨模态行人检测方法, CN110826392A, 郑爱华; 邹甜; 王逍; 王梓; 罗斌; 汤进, 2020.02.21
<li><p> 一种基于时空线索的跨模态视频显著性检测方法, CN109034001A, 汤进; 范东哲; 李成龙; 王逍, 2018.12.18 
<li><p> 一种基于困难正样本生成的目标跟踪方法, CN108596958A, 李成龙; 杨芮; 王逍; 汤进; 罗斌, 2018.09.28
</ul>
Searched via <a href="https://pss-system.cponline.cnipa.gov.cn/conventionalSearch">[https://pss-system.cponline.cnipa.gov.cn/conventionalSearch]</a>


<h2>Laboratory Memories</h2> 

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image Slideshow</title>
<style>
  .slideshow {
    display: flex;
    flex-direction: column;
    width: 800px; /* 设置适当的宽度 */
    height: 600px; /* 设置适当的高度 */
    overflow: hidden;
    position: relative;
  }
  .slideshow img {
    width: 100%;
    height: 100%;
    position: absolute;
    top: 0;
    left: 0;
    object-fit: cover; /* 保持原始比例，不缩放 */
  }
  .slideshow button {
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    z-index: 1; /* 确保按钮在图像之上 */
    width: 80px; /* 设置按钮宽度为 80px */
    height: 20px; /* 设置按钮高度为 20px */
  }
  .slideshow #previous {
    left: 10px; /* 将按钮放置在图像左侧 */
    bottom: 10px; /* 将按钮放置在图像底部 */
  }
  .slideshow #next {
    right: 10px; /* 将按钮放置在图像右侧 */
    bottom: 10px; /* 将按钮放置在图像底部 */
  }
</style>
</head>
<body>
<div class="slideshow">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/AAAI2024_001.jpg" alt="Image 1">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/AAAI2024_004.jpg" alt="Image 3">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/datacapture002.jpg" alt="Image 8">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/lab001.jpg" alt="Image 9">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/lab003.jpg" alt="Image 11">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/lab004.jpg" alt="Image 13">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/lab006.jpg" alt="Image 14">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/prcv2023.jpg" alt="Image 15">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/student001.jpg" alt="Image 16">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/student002.jpg" alt="Image 17">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/student003.jpg" alt="Image 18">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/student006.jpg" alt="Image 21">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/student007.jpg" alt="Image 22">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/talk001.jpg" alt="Image 24">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/talk004.jpg" alt="Image 26">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/talk005.jpg" alt="Image 27">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/valse2024_001.jpg" alt="Image 29">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/valse2024_002.jpg" alt="Image 30">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/valse2024_shiaowang.jpg" alt="Image 31">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/xiaowang002.jpg" alt="Image 33">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/xiaowang003.jpg" alt="Image 34">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/xiaowang004.jpg" alt="Image 35">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/PosterValse2024.jpg" alt="Image 36">	
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/xiaowang005.png" alt="Image 37">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/risingSTAR_award_XiaoWang.jpg" alt="Image 38">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/xiaowang20240701.jpg" alt="Image 39">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/xiaowang20240702.jpg" alt="Image 40">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/HefeiDclass.png" alt="Image 41">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/XiaoWangatPCL1.jpg" alt="Image 42">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/csig_PAR_talk_news.jpg" alt="Image 43">
  <img src="https://raw.githubusercontent.com/wangxiao5791509/wangxiao5791509.github.io/main/sides_play/ciyueshu_workshop.jpg" alt="Image 44">	
  <button id="previous">Previous</button> 
  <button id="next">Next</button> 
  <script src="slideshow.js"></script> 
</div>
</body>





<p><li> The album can play automatically, please wait a moment. Alternatively, you can click [Previous] [Next] to view different photos. </p> 
<p><li> The wedding blessing video students made for me. I like it very much! <a href="https://youtu.be/Vt5yuF7mQZE?si=UySmq1wAgBEEOzHO">[Youtube]</a> </p> 






<h2>Map My Visitors</h2>
<script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=a&t=m&d=wonbM2xxjD4O1OiNs-4xysKwM_FSwud0uY2QagF7RIM&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>  
</ul>

  
  
  

  
  
