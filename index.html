<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiao Wang</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Xiao Wang (王逍)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://wangxiao5791509.github.io/"><img src="picture/xiaowang.JPG" alt="alt text" width="140px" /></a>&nbsp;</td>
<td align="left"><p>Associate Professor<br />
School of Computer Science and Technology <br />
Anhui University <br />
Hefei City, Anhui Province, China <br />
Email: wangxiaocvpr@foxmail.com; xiaowang@ahu.edu.cn <br />
<br />
<a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F5y9bxxP-FRWEf7aiRuT51F8TV9OiT1uk7E-Ak0IMOTvmHoW6B5gJwLsBJ6tkj1XunyQWh2rhbfQLf-cUo6gZb6mgFO33k9D_qfPqCTyivEjOmXZuM&user=oq9awGMAAAAJ">[Google Scholar]</a> 
<a href="https://github.com/wangxiao5791509">[GitHub]</a> 
<a href="https://www.zhihu.com/people/wangxiaocvpr">[Zhihu]</a>
<a href="https://dblp.uni-trier.de/pid/49/67-14.html">[DBLP]</a>
<a href="https://orcid.org/0000-0001-6117-6745">[ORCID]</a>
<a href="https://www.cnblogs.com/wangxiaocvpr/">[Blog]</a>
<a href="https://weibo.com/5070353058">[Weibo]</a>
<a href="https://www.youtube.com/channel/UCSmlgXQRqFsUI--mQm90JwQ">[YouTube]</a>
<a href="https://github.com/Event-AHU">[Event-AHU]</a> 
</td>

</tr></table>

<h2>Biography</h2>
<p>I received the Ph.D. degree from <a href="http://cs.ahu.edu.cn/">[<b>Anhui University</b>]</a>, 
  Hefei 230601, China, in 2019. In my Ph.D. career, I was supervised by Professor <b>Jin Tang</b> and <b>Bin Luo</b> 
  (Lab: <a href="https://mcc.ahu.edu.cn/">[<b>Multi-Modal Intelligent Computing Group</b>]</a>). 
  From 2015 and 2016, I was a visiting student at the School of Data and Computer Science, <b>Sun Yat-sen University</b>, Guangzhou, China, 
  and supervised by Professor <a href="http://www.linliang.net/">[<b>Liang Lin</b>]</a>. 
  From 7 July to 20 November 2019, I was studying in the J17 Sydney Data Science Hub, <b>University of Sydney</b>, 
  and supervised by Professor <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">[<b>Dacheng Tao</b>]</a>. 
  From 9 April 2020 to 8 April 2022, I begin my Postdoc career at <a href="https://www.pcl.ac.cn/">[<b>Pengcheng Laboratory</b>]</a> (Shenzhen, China) 
  and cooperate with Professor 
  <a href="https://eeis.ustc.edu.cn/2014/0423/c2648a20109/page.htm">[<b>Feng Wu</b>]</a>, 
  <a href="https://www.pkuml.org/staff/yhtian.html">[<b>Yonghong Tian</b>]</a>, and 
  <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ&hl=zh-CN&oi=ao">[<b>Yaowei Wang</b>]</a>. 
  Currently, I work at the <a href="http://cs.ahu.edu.cn/">[School of Computer Science and Technology]</a> of <b>Anhui University</b>, 
  Hefei, China, as an Associate Professor. 

<p>My current research interests are mainly in vision-based artificial intelligence and pattern recognition.  
  I am a reviewer for IEEE TCSVT, TIP, IJCV, CVIU, PR, CVPR, ICCV, AAAI, ECCV, ICLR, ACCV, ACM-MM, and WACV. </p>


<p><font color="#FF0000"><b>欢迎对科研感兴趣，编程能力强，基础扎实，善于思考和钻研，认真负责的学生报考本人研究生；欢迎学有余力的本科生参与科研项目实践；有意者请随时与我联系!</b></font> <br /> 
  
  

   
  
  
<h2>Research Topic</h2>
<ul>
<li><p><b>Object Tracking</b>: Single-, Multi-Object Tracking  
<li><p><b>Multi-modal</b>: Vision-Language, Vision-Thermal (Infrared), Vision-Depth, Vision-Event, Vision-Audio  
<li><p><b>Neuromorphic Vision</b>: Detection and Tracking, Action Recognition, Scene Reconstruction
</ul>
  

  
  
  
  
  
<h2>Selected Publications</h2> 



  

<ul> 
<li><p><a href="">Unified and Dynamic Graph for Temporal Character Grouping in Long Videos</a> <br />
 Xiujun Shu, Wei Wen, Liangsheng Xu, Mingbao Lin, Ruizhi Qiao, Taian Guo, Hanjun Li, Bei Gan, <b>Xiao Wang</b>, Xin Sun <br />
<i>arXiv 2308.14105  
  <a href="https://arxiv.org/pdf/2308.14105.pdf">[arXiv]</a>
</i></p>
</li>
</ul>

  

<ul> 
<li><p><a href="">Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification</a> <br />
 Chengguo Yuan, Yu Jin, Zongzhen Wu, Fanting Wei, Yangzirui Wang, Lan Chen, and <b>Xiao Wang</b> <br />
<i>PRCV-2013  
  <a href="https://arxiv.org/abs/2308.11937">[arXiv]</a>
  <a href="https://github.com/Event-AHU/EFV_event_classification">[Github]</a>
</i></p>
</li>
</ul>




<ul> 
<li><p><a href="">SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition</a> <br />
 <b>Xiao Wang</b>, Zongzhen Wu, Yao Rong, Lin Zhu, Bo Jiang, Jin Tang, Yonghong Tian <br />
<i>arXiv:2308.04369, 2023  
  <a href="https://arxiv.org/abs/2308.04369">[arXiv]</a>
  <a href="https://github.com/Event-AHU/SSTFormer">[Github]</a>
</i></p>
</li>
</ul>

  
<ul> 
<li><p><a href="">Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition</a> <br />
 Bo Jiang, Chengguo Yuan, <b>Xiao Wang*</b>, Zhimin Bao, Lin Zhu, Yonghong Tian, Jin Tang <br />
<i>arXiv:2306.05239, 2023  
  <a href="https://arxiv.org/abs/2306.05239">[arXiv]</a>
  <a href="https://github.com/Event-AHU/AGCN_Event_Classification">[Github]</a>
</i></p>
</li>
</ul>
  
  
<ul> 
<li><p><a href="">Deep Triply Attention Network for RGBT Tracking</a> <br />
 Rui Yang, <b>Xiao Wang</b>, Yabin Zhu, Jin Tang <br />
<i>Cognitive Computation 2023  
  <a href="https://trebuchet.public.springernature.app/get_content/ddb79356-e4e2-4f50-a461-3a0532cfbb93">[Paper]</a>
</i></p>
</li>
</ul> 
  
  
<ul> 
<li><p><a href="">AMatFormer: Efficient Feature Matching via Anchor Matching Transformer</a> <br />
 Bo Jiang, Shuxian Luo, <b>Xiao Wang*</b>, Chuanfu Li and Jin Tang <br />
<i>IEEE Transactions on Multimedia (TMM) 2023  
  <a href="https://arxiv.org/abs/2305.19205">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10143284/">[IEEE]</a>
</i></p>
</li>
</ul> 

  
  
<ul> 
<li><p><a href="">Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition</a> <br />
 Jun Zhu†, Jiandong Jin†, Zihan Yang, Xiaohao Wu, <b>Xiao Wang*</b> († denotes equal contribution) <br />
<i>CVPR-2023 Workshop@NFVLR (New Frontiers in Visual Language Reasoning: Compositionality, Prompts and Causality) 
  <a href="https://arxiv.org/abs/2304.10091">[arXiv]</a>
  <a href="https://openaccess.thecvf.com/content/CVPR2023W/NFVLR/papers/Zhu_Learning_CLIP_Guided_Visual-Text_Fusion_Transformer_for_Video-Based_Pedestrian_Attribute_CVPRW_2023_paper.pdf">[CVF]</a>
  <a href="https://github.com/Event-AHU/VTF_PAR">[Github]</a>
</i></p>
</li>
</ul> 


  
<ul> 
<li><p><a href="">RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided Learning</a> <br />
 Yabin Zhu, Chenglong Li, <b>Xiao Wang</b>, Jin Tang, and Zhixiang Huang <br />
<i>arXiv 2023 pre-print, arXiv:2303.14778 
  <a href="https://arxiv.org/pdf/2303.14778.pdf">[arXiv]</a>
</i></p>
</li>
</ul> 
  
  


  
<ul> 
<li><p><a href="">Transformer Vision-Language Tracking via Proxy Token Guided Cross-Modal Fusion</a> <br />
 Haojie Zhao, <b>Xiao Wang</b>, Dong Wang, Huchuan Lu, and Xiang Ruan <br />
<i>Pattern Recognition Letters (PRL), Elsevier, 2023 
  <a href="">[arXiv]</a>
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865523000545">[PRL]</a>
  <a href="">[GitHub]</a>
</i></p>
</li>
</ul> 
  
  

  
<ul> 
<li><p><a href="">Rethinking Batch Sample Relationships for Data Representation: A Batch-Graph Transformer based Approach</a> <br />
 Xixi Wang, Bo Jiang, <b>Xiao Wang</b>, Bin Luo <br />
<i>IEEE Transactions on Multimedia (TMM) 2023, 
  <a href="https://arxiv.org/abs/2211.10622">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10144585">[IEEE]</a>
</i></p>
</li>
</ul>    
 
  
  
<ul> 
<li><p><a href="">Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric</a> <br />
 Chuanming Tang, <b>Xiao Wang*</b>, Ju Huang, Bo Jiang, Lin Zhu, Jianlin Zhang*, Yaowei Wang, Yonghong Tian <br />
<i>arXiv pre-print, arXiv:2211.11010 
  <a href="https://arxiv.org/abs/2211.11010">[arXiv]</a>
  <a href="https://github.com/Event-AHU/COESOT">[GitHub]</a>
  <a href="https://youtu.be/_ROv09rvi2k">[DemoVideo]</a>
  <a href="https://sites.google.com/view/coesot/">[Project]</a>
</i></p>
</li>
</ul>  


  
  

<ul> 
<li><p><a href="">HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors</a> <br />
 <b>Xiao Wang</b>, Zongzhen Wu, Bo Jiang, Zhimin Bao, Lin Zhu, Guoqi Li, Yaowei Wang, Yonghong Tian <br />
<i>arXiv pre-print, arXiv:2211.09648 
  <a href="https://arxiv.org/abs/2211.09648">[arXiv]</a>
  <a href="https://github.com/Event-AHU/HARDVS">[GitHub]</a>
  <a href="https://youtu.be/AgYjh-pfUT0">[DemoVideo]</a>
  <a href="https://sites.google.com/view/hardvs/">[Project]</a>
</i></p>
</li>
</ul>
  
  



<ul> 
<li><p><a href="">Few-Shot Learning Meets Transformer: Unified Query-Support Transformers for Few-Shot Classification</a> <br />
  Xixi Wang, <b>Xiao Wang</b>, Bo Jiang, Bin Luo <br />
<i> IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2023 
  <a href="https://arxiv.org/abs/2208.12398">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/abstract/document/10144072/">[IEEE]</a>
</i></p>
</li>
</ul>


  
<ul> 
<li><p><a href="">Learning Spatial-Frequency Transformer for Visual Object Tracking</a> <br />
  Chuanming Tang, <b>Xiao Wang*</b>, Yuanchao Bai, Zhe Wu, Jianlin Zhang, Yongmei Huang <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023  
  <a href="https://arxiv.org/abs/2208.08829">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10054166">[IEEE]</a>
  <a href="https://github.com/Tchuanm/SFTransT">[Github]</a> 
</i></p>
</li>
</ul>

  

<ul> 
<li><p><a href="">Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey</a> <br />
 <b>Xiao Wang</b>, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, Wen Gao <br />
<i>Machine Intelligence Research (MIR), 2022 
  <a href="https://arxiv.org/abs/2302.10035">[arXiv]</a>
  <a href="https://www.mi-research.net/article/doi/10.1007/s11633-022-1410-8">[MIR]</a> 
  <a href="https://mp.weixin.qq.com/s/5eELXfACI67yZT7WUtMFMA">[极市平台公众号]</a> 
  <a href="https://mp.weixin.qq.com/s/yX1DdDCA-nMluzOB6Qz3sw">[ MIR编辑部@机器智能研究MIR]</a> 
  <a href="https://github.com/wangxiao5791509/MultiModal_BigModels_Survey">[Github]</a> 
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="">See Finer, See More: Implicit Modality Alignment for Text-based Person Retrieval</a> <br />
Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song, Ruizhi Qiao, Bo Ren, <b>Xiao Wang*</b> <br />
<i>The 2nd Workshop on Real-World Surveillance: Applications and Challenges, ECCVW-2022
  <a href="https://arxiv.org/abs/2208.08608">[arXiv]</a>
  <a href="https://vap.aau.dk/rws-eccv2022/">[ECCV]</a>
  <a href="https://youtu.be/h8zCTLjJAxM">[YouTube]</a>
  <a href="https://github.com/shuxjweb/IVT">[Code]</a>
</i></p>
</li>
</ul>
  
  





<ul>
<li><p><a href="https://arxiv.org/abs/2207.12767">Criteria Comparative Learning for Real-scene Image Super-Resolution</a> <br />
Yukai Shi, Hao Li, Sen Zhang, Zhijing Yang, <b>Xiao Wang</b> <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology, 2022 
  <a href="https://arxiv.org/abs/2207.12767">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9847265">[IEEE]</a>
  <a href="https://github.com/House-Leo/RealSR-CCL">[Code]</a> 
  <a href="https://github.com/House-Leo/RealSR-Zero">[Dataset]</a> 
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2205.13125">Prompt-based Learning for Unpaired Image Captioning</a> <br />
Peipei Zhu, <b>Xiao Wang</b>, Lin Zhu, Zhenglong Sun, Weishi Zheng, Yaowei Wang, Changwen Chen <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2205.13125">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/10097833">[IEEE]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2107.10433">MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking</a> <br />
<b>Xiao Wang</b>, Xiujun Shu, Shiliang Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2107.10433">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9772993">[IEEE]</a>
  <a href="https://github.com/wangxiao5791509/MFG_RGBT_Tracking_PyTorch">[Code]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2203.03195">Unpaired Image Captioning by Image-level Weakly-Supervised Visual Concept Recognition</a> <br />
Peipei Zhu, <b>Xiao Wang</b>, Yong Luo, Zhenglong Sun, Wei-Shi Zheng, Yaowei Wang, Changwen Chen <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2203.03195">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9917391">[IEEE]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/abs/2202.05659">Tiny Object Tracking: A Large-scale Dataset and A Baseline</a> <br />
Yabin Zhu, Chenglong Li, Yao Liu, <b>Xiao Wang</b>, Jin Tang, Bin Luo, Zhixiang Huang <br />
<i>IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS), 2023. 
<a href="https://arxiv.org/abs/2202.05659">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/10035907">[IEEE]</a>
<a href="https://github.com/ZYB0726/MKDNet">[Dataset and Evaluation Toolkit]</a> 
</i></p>
</li>
</ul>

  
  

  
<ul>
<li><p><a href="https://arxiv.org/pdf/2201.10943.pdf">Event-based Video Reconstruction via Potential-assisted Spiking Neural Network</a> <br />
Lin Zhu, <b>Xiao Wang</b>, Yi Chang, Jianing Li, Tiejun Huang, Yonghong Tian <br />
<i>CVPR, 2022. <a href="https://arxiv.org/pdf/2201.10943.pdf">[arXiv]</a>
  <a href="https://github.com/LinZhu111/EVSNN">[Code]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://arxiv.org/abs/2112.01177v3">MutualFormer: Multi-Modality Representation Learning via Cross-Diffusion Attention</a> <br />
Xixi Wang, <b>Xiao Wang</b>, Bo Jiang*, Jin Tang and Bin Luo <br />
<i>in peer review, 2021. 
<a href="https://arxiv.org/pdf/2112.01177v3.pdf">[arXiv]</a>
<a href="https://github.com/SissiW/MutualFormer">[GitHub]</a>
</i></p>
</li>
</ul>
  
  
   
    
  
  

  
<ul>
<li><p><a href="https://www.aaai.org/AAAI22Papers/AAAI-1396.LiJ.pdf">Retinomorphic Object Detection in Asynchronous Visual Streams</a> <br />
Jianing Li+, <b>Xiao Wang+</b>, Lin Zhu, Jia Li, Tiejun Huang, Yonghong Tian (+ denotes equal contribution) <br />
<i>AAAI, 2022, <font color="#FF0000">Oral representation</font>. 
<a href="https://www.aaai.org/AAAI22Papers/AAAI-1396.LiJ.pdf">[PDF]</a>
<a href="https://aaai-2022.virtualchair.net/poster_aaai1396">[Poster]</a>
<a href="https://www.pkuml.org/resources/pku-vidar-dvs.html">[Project Page]</a>
<a href="https://git.openi.org.cn/lijianing/PKU-Vidar-DVS/datasets">[Dataset]</a>
</i></p>
</li>
</ul>
  
  
  
   
  
<ul>
<li><p><a href="https://arxiv.org/abs/2108.05015">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows</a> <br />
<b>Xiao Wang</b>, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>in peer review, 2021. 
<a href="https://arxiv.org/abs/2108.05015">[arXiv]</a>
<a href="https://sites.google.com/view/viseventtrack/">[Project]</a>
<a href="https://www.youtube.com/watch?v=U4uUjci9Gjc&ab_channel=XiaoWang">[Demo]</a>
<a href="https://www.youtube.com/watch?v=vGwHI2d2AX0&ab_channel=XiaoWang">[Video Tutorial]</a>
<a href="https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark">[Dataset and Evaluation Toolkit]</a>
<a href="https://github.com/wangxiao5791509/RGB-DVS-SOT-Baselines">[Source Code]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2205.09676">Beyond Greedy Search: Tracking by Multi-Agent Reinforcement Learning-based Beam Search</a> <br />
<b>Xiao Wang</b>, Zhe Chen, Bo Jiang, Jin Tang, Bin Luo, Dacheng Tao <br />
<i>IEEE Transactions on Image Processing (TIP), 2022. 
<a href="https://arxiv.org/abs/2205.09676">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/9904473">[IEEE]</a>
<a href="https://sites.google.com/view/beamtracking/">[Project]</a>
<a href="https://www.youtube.com/watch?v=f1yiYv-SJyY&ab_channel=XiaoWang">[Video]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2205.09676">RGBT Tracking via Cross-modality Message Passing</a> <br />
Rui Yang, <b>Xiao Wang</b>, Chenglong Li, Jinmin Hu, Jin Tang <br />
<i>Neurocomputing, 2021. 
<a href="https://www.sciencedirect.com/science/article/pii/S0925231221011966">[PDF]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/abs/2108.00803">Learn to Match: Automatic Matching Network Design for Visual Tracking</a> <br />
Zhipeng Zhang, Yihao Liu, <b>Xiao Wang</b>, Bing Li, Weiming Hu <br />
<i>ICCV, 2021. 
<a href="https://arxiv.org/abs/2108.00803">[arXiv]</a>
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learn_To_Match_Automatic_Matching_Network_Design_for_Visual_Tracking_ICCV_2021_paper.pdf">[ICCV]</a>
<a href="https://github.com/JudasDie/SOTS">[Code]</a>
<a href="https://www.youtube.com/watch?v=iABDcmi3gVI&ab_channel=XiaoWang">[Video]</a>
<a href="https://drive.google.com/file/d/1CcBuGyivdSmvp8dqtrt4kttQEsrkQETG/view">[Poster]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf">NeuSpike-Net: High Speed Image Reconstruction via Bio-inspired Neuromorphic Cameras</a> <br />
Lin Zhu, Jianing Li, <b>Xiao Wang</b>, Tiejun Huang, Yonghong Tian <br />
<i>ICCV, 2021. 
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf">[PDF]</a>
<a href="https://sites.google.com/view/retina-recon">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9463711">Semantic-guided Pixel Sampling for Cloth-Changing Person Re-identification</a> <br />
Xiujun Shu, Ge Li, <b>Xiao Wang</b>, Weijian Ruan, Qi Tian <br />
<i>IEEE Signal Processing Letters, 2021. 
<a href="https://ieeexplore.ieee.org/document/9463711">[IEEE]</a>
<a href="https://arxiv.org/pdf/2107.11522.pdf">[arXiv]</a>
<a href="https://github.com/shuxjweb/pixel_sampling">[Code]</a>
</i></p>
</li>
</ul> 

  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9615085">Large-Scale Spatio-Temporal Person Re-identification: Algorithms and Benchmark</a> <br />
Xiujun Shu+, <b>Xiao Wang+</b>, Shiliang Zhang, Xianghao Zhang, Yuanqi Chen, Ge Li, Qi Tian (+ denotes equal contribution) <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021. 
<a href="https://ieeexplore.ieee.org/abstract/document/9615085">[IEEE]</a>
<a href="https://arxiv.org/pdf/2105.15076.pdf">[arXiv]</a>
<a href="https://sites.google.com/view/personreid">[Project]</a>
<a href="https://github.com/shuxjweb/last">[Code]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2103.16746">Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark</a> <br />
<b>Xiao Wang+</b>, Xiujun Shu+, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu (+ denotes equal contribution) <br />
<i>CVPR, 2021. 
<a href="https://arxiv.org/abs/2103.16746">[arXiv]</a>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Towards_More_Flexible_and_Accurate_Object_Tracking_With_Natural_Language_CVPR_2021_paper.pdf">[CVF]</a>
<a href="https://sites.google.com/view/langtrackbenchmark/">[Project]</a>
<a href="https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit">[EvaluationToolkit]</a>
<a href="https://www.youtube.com/watch?v=7lvVDlkkff0&ab_channel=XiaoWang">[Demo]</a>
<a href="https://www.youtube.com/watch?v=uE2qnNf-ClI&ab_channel=XiaoWang">[Tutorial]</a>
</i></p>
</li>
</ul>
  


<ul>
<li><p><a href="https://arxiv.org/abs/2106.04840">Tracking by Joint Local and Global Search: A Target-aware Attention based Approach</a> <br />
<b>Xiao Wang</b>, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, and Feng Wu <br />
<i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021. 
<a href="https://arxiv.org/abs/2106.04840">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/9511641">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/LGSearch_DDGAN_PyTorch">[Code]</a>
<a href="https://www.youtube.com/watch?v=uu00QIL7tjo&ab_channel=XiaoWang">[Tutorial]</a>
</i></p>
</li>
</ul>
  
  

<ul>
<li><p><a href="https://arxiv.org/pdf/2007.03584.pdf">STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification</a> <br />
Bo Jiang, Sheng Wang, <b>Xiao Wang</b>*, Aihua Zheng (* denotes corresponding author) <br />
<i>arxiv preprint arXiv:2007.03584, 2021. 
<a href="https://arxiv.org/pdf/2007.03584.pdf">[arXiv]</a>
<a href="https://github.com/wangxiao5791509/STADB_ReID">[Code]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.10280.pdf">cmSalGAN: RGB-D Salient Object Detection with Cross-View Generative Adversarial Networks</a> <br />
Bo Jiang, Zitai Zhou, <b>Xiao Wang*</b>, Jin Tang, Bin Luo (* denotes corresponding author) <br />
<i>IEEE Transactions on Multimedia (TMM), 2020. 
<a href="https://arxiv.org/pdf/1912.10280.pdf">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/9103135">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/cmSalGAN_PyTorch">[Code]</a>
<a href="https://sites.google.com/view/cmsalgan/">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9345930/">Dynamic Attention-guided Multi-Trajectory Analysis for Single Object Tracking</a> <br />
<b>Xiao Wang</b>, Zhe Chen, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021. 
<a href="https://arxiv.org/abs/2103.16086">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/9345930/">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/DeepMTA_PyTorch">[Code]</a>
<a href="https://sites.google.com/view/mt-track/home/">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/1811.10014">Describe and Attend to Track: Learning Natural Language guided Structural Representation and Visual Attention for Object Tracking</a> <br />
<b>Xiao Wang</b>, Chenglong Li, Rui Yang, Tianzhu Zhang, Jin Tang, Bin Luo <br />
<i>arxiv, preprint, arXiv:1811.10014, 2018. 
<a href="https://arxiv.org/abs/1811.10014">[arXiv]</a>
<a href="https://sites.google.com/view/languagetracking/">[Project]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0923596518308683">Quality–aware dual–modal saliency detection via deep reinforcement learning</a> <br />
<b>Xiao Wang</b>, Tao Sun, Rui Yang, Chenglong Li, Bin Luo, Jin Tang <br />
<i>Signal Processing and Image Communication, 2019. 
<a href="https://www.sciencedirect.com/science/article/pii/S0923596518308683">[PDF]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://bmvc2019.org/wp-content/uploads/papers/0562-paper.pdf">Learning Target-aware Attention for Robust Tracking with Conditional Adversarial Network</a> <br />
<b>Xiao Wang</b>, Tao Sun,  Rui Yang, Bin Luo  <br />
<i>30TH British Machine Vision Conference (BMVC), 2019. 
<a href="https://bmvc2019.org/wp-content/uploads/papers/0562-paper.pdf">[PDF]</a>
<a href="https://bmvc2019.org/wp-content/uploads/papers/0562-supplementary.pdf">[Supplementary]</a>
<a href="https://sites.google.com/view/globalattentiontracking">[Project]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0031320321004015">Pedestrian Attribute Recognition: A Survey</a> <br />
<b>Xiao Wang</b>, Shaofei Zheng, Rui Yang, Aihua Zheng, Zhe Chen, Bin Luo, Jin Tang  <br />
<i>Pattern Recognition, 2021. 
<a href="https://arxiv.org/abs/1901.07474">[arXiv]</a>
<a href="https://www.sciencedirect.com/science/article/pii/S0031320321004015">[Pattern Recognition]</a>
<a href="https://sites.google.com/view/ahu-pedestrianattributes/">[Project]</a>
<a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List">[Github]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/pdf/1908.04441.pdf">Learning Target-oriented Dual Attention for Robust RGB-T Tracking</a> <br />
Rui Yang, Yabin Zhu, <b>Xiao Wang</b>, Chenglong Li, Jin Tang  <br />
<i>IEEE International Conference on Image Processing (ICIP), 2019. 
<a href="https://arxiv.org/pdf/1908.04441.pdf">[arXiv]</a>
</i></p>
</li>
</ul>

  

<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S092523121931255X">Multi-modal foreground detection via inter- and intra-modality-consistent low-rank separation</a> <br />
Aihua Zheng,  Naipeng Ye, Chenglong Li, <b>Xiao Wang</b>, Jin Tang <br />
<i>Neurocomputing, 2020. 
<a href="https://www.sciencedirect.com/science/article/pii/S092523121931255X">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="Dense Feature Aggregation and Pruning for RGBT Tracking">Dense Feature Aggregation and Pruning for RGBT Tracking</a> <br />
Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, <b>Xiao Wang</b>  <br />
<i>Proceedings of the 27th ACM International Conference on Multimedia (ACM MM), 2019
<a href="https://arxiv.org/abs/1907.10451">[arXiv]</a>
<a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350928?casa_token=X-2D-Vf5XNIAAAAA:FP95TAXyz0gMYafxBEwbLCxG-fVXNFN8MC2TUPx1BxQsiFD4CgArbxObOQcSa-WA4MIAeWHAe8cKktk">[ACM MM]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation</a> <br />
<b>Xiao Wang</b>, Chenglong Li, Bin Luo, Jin Tang <br />
<i>CVPR, 2018
<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">[PDF]</a>
<a href="https://sites.google.com/view/cvpr2018sintplusplus/">[Project]</a>
</i></p>
</li>
</ul>
  


<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/7457366">Weighted Low-Rank Decomposition for Robust Grayscale-Thermal Foreground Detection</a> <br />
Chenglong Li+, <b>Xiao Wang+</b>, Lei Zhang, Jin Tang, Hejun Wu, Liang Lin (+ denotes equal contribution) <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017
<a href="https://ieeexplore.ieee.org/document/7457366">[PDF]</a>
<a href="https://sites.google.com/view/mmmovingobjectdetectiontip/">[Project]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7937922">Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning</a> <br />
Ziliang Chen, Keze Wang, <b>Xiao Wang</b>, Pai Peng, Ebroul Izquierdo, Liang Lin  <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017
<a href="https://ieeexplore.ieee.org/abstract/document/7937922">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7822984/">Grayscale-Thermal Object Tracking via Multitask Laplacian Sparse Representation</a> <br />
Li, Chenglong and Sun, Xiang and <b>Xiao Wang</b> and Zhang, Lei and Tang, Jin <br />
<i>IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2017
<a href="https://ieeexplore.ieee.org/abstract/document/7822984/">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
<h2>Class Schedule</h2>
<ul>
<li><p>2022-2023 秋学期    MATLAB 编程基础 <br />
<li><p>2022-2023 秋学期    数据挖掘与知识发现 <br />
<li><p>2022-2023 春学期    机器学习 <br /> 
<li><p>2022-2023 春学期    计算机伦理 <br /> 
<li><p>2023-2024 秋学期    深度学习（理论及实验课） <br /> 
<li><p>2023-2024 秋学期    计算机伦理 <br /> 
</ul>

  
  
  
  
<h2>Video Tutorial</h2>
<ul>
<li><p>浅谈科研论文写作 (2023-01-09) <a href="https://youtu.be/GK0Pq0fSe0M">[YouTube]</a> <br />
</ul>
  

  
  
  

<h2>Awards</h2>
<ul>
<li><p>2014-2015    National Academic Scholarships <br />
<li><p>2015-2016    National Academic Scholarships <br />
<li><p>2016-2017    National Academic Scholarships <br />
<li><p>2017-2018    National Academic Scholarships <br />
<li><p>2018-2019    National Academic Scholarships <br />
<li><p>2018-2019    GuoYuan Scholarships <br />
<li><p>2019		安徽省校级优秀毕业生 AHU-University graduates <br /> 
<li><p>2019		<font color="#FF0000">安徽省省级优秀毕业生 AnHui Outstanding Graduates </font> <br /> 
<li><p>2022		<font color="#FF0000">安徽省合肥市D类高层次人才(市级领军人才)</font> <br /> 
</ul>
  
  

  
  
<h2>Academic Activities</h2>
<ul>  
<li><p>2022.05.22    IEEE Member (Member number: 96976176)
<li><p>2022.05.30    CSIG Member (Member number: E654403548M)
</ul>
  
  
  

<h2>Funds and Projects</h2>
<ul>
<li><p>Postdoctoral Innovative Talent Support Program (BX20200174, 2021-2022) <br />
<li><p>China Postdoctoral Science Foundation Funded Project (2020M682828, 2021-2022) <br />
<li><p>National Natural Science Foundation of China (编号：62102205, 名称：面向神经形态视觉的目标跟踪关键技术研究, 2022-2024) <br />

  
  
  
<br />
</ul>
<div id="footer">
<div id="footer-text">
<br>Page generated 2022-11-24, by <a href="https://wangxiao5791509.github.io/">Xiao Wang, School of Computer Science and Technology, Anhui University (安徽大学-计算机科学与技术学院-王逍)</a>.
</div>
</div>
</div>
</body>
</html>

  
  
  
  

  
  
  
  
  
  
